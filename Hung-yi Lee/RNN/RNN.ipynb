{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作业要求：接触NLP 当中一个简单的task —— 语句分类（文本分类），给定一个语句，判断他有没有恶意（负面标1，正面标0）\n",
    "# 环境：Anaconda3+Jupyter Notebook，Python 3.6.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.utils.py\n",
    "# 这个block用来定义一下用到的函数\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')# 过滤警告\n",
    "import torch # 深度学习框架\n",
    "import numpy as np # 支持大量的维度数组与矩阵运算\n",
    "import pandas as pd # 用于数据分析\n",
    "import torch.optim as optim # 主要包含用来更新参数的优化算法\n",
    "import torch.nn.functional as F # 主要包含用来搭建各个层的模块和一系列有用的loss函数\n",
    "\n",
    "# 把training 时需要的data读取出来\n",
    "def load_training_data(path='training_label.txt'): \n",
    "    # 如果是‘training_label.txt',需要读取label；如果是’training_nolabel.txt'，不需要读取label\n",
    "    if 'training_label' in path:\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            lines=f.readlines()\n",
    "            lines=[line.strip('\\n').split(' ') for line in lines] \n",
    "            # line为lines中删除开头结尾处的‘\\n’并按照‘  ’分割\n",
    "        x=[line[2:] for line in lines]\n",
    "        y=[line[0] for line in lines]\n",
    "        return x,y\n",
    "    else:\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            lines=f.readlines()\n",
    "            x=[line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "    \n",
    "# 把 testing 时需要的data读取出来    \n",
    "def load_testing_data(path='testing_data.txt'):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "        # 将lines中的line删除开头结尾处的‘\\n’并按‘ ’分割加入新的空字符串中，再删除空白符\n",
    "        X=[\"\".join(line.strip('\\n').split(',')[1:]).strip() for line in lines]\n",
    "        # 取出X中的元素并以‘ ’分割\n",
    "        X=[sen.split(' ') for sen in X]\n",
    "    return X\n",
    "\n",
    "# outputs  => 概率(float)  labels   => labels\n",
    "def evaluation(outputs,labels):\n",
    "    # 大于等于0.5即为正面\n",
    "    outputs[outputs>=0.5]=1\n",
    "    # 小于0.5为负面\n",
    "    outputs[outputs<0.5]=0 \n",
    "    # torch.sum(input)对输入的tensor数据的某一维度求和\n",
    "    # torch.eq(input,other)比较元素是否相等，other可以是一个数或者是一个和input同类型形状的张量\n",
    "    #        返回结果是一个torch.ByteTensor张量，包含了每个位置的比较结果（相等为1，不等为0）\n",
    "    # correct即为将outputs和labels比较后的形成的tensor中的元素\n",
    "    correct=torch.sum(torch.eq(outputs,labels)).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading testing data ...\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "# 2.w2v.py \n",
    "# 这个block将训练word变成vector做word embedding\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse # python命令行解析模块\n",
    "from gensim.models import Word2Vec  #用于初始化并训练一个单词到向量转换\n",
    "\n",
    "# 训练 word to vector 的 word embedding词嵌入\n",
    "def train_word2vec(x):    \n",
    "    # 参数说明：\n",
    "    # x---语料集，size---特征向量的维度，\n",
    "    # window---表示当前词与预测词在一个句子中最大的距离\n",
    "    # min_count---对字典做截断，词频少于min_count次数的单词会被丢弃掉\n",
    "    # workers---控制训练的并行数\n",
    "    # iter---迭代次数\n",
    "    # sg---设置训练算法 默认为0---CBOW算法，sg=1则采用skip-gram算法\n",
    "    model = Word2Vec(x,size=250,window=5,min_count=5,workers=12,iter=10,sg=1)\n",
    "    return model\n",
    "\n",
    "# 此部分的代码入口在此\n",
    "if __name__==\"__main__\": \n",
    "    # 加载训练数据\n",
    "    print(\"loading training data ...\")\n",
    "    train_x,y=load_training_data('training_label.txt')\n",
    "    train_x_no_label=load_training_data('training_nolabel.txt')\n",
    "    \n",
    "    # 加载测试数据\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('testing_data.txt')\n",
    "    \n",
    "    # 将word转换成vector\n",
    "    #model = train_word2vec(train_x+train_x_no_label+test_x)\n",
    "    model=train_word2vec(train_x+test_x)\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    #model.save(os.path.join('./','model/w2v_all.model'))\n",
    "    model.save(os.path.join('./','w2v_all.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "# 这个block用来做data的预处理\n",
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        self.w2v_path = w2v_path # word2vetor模型路径\n",
    "        self.sentences = sentences # 句子\n",
    "        self.sen_len = sen_len # 句子长度\n",
    "        self.idx2word = [] # idx 到 单词\n",
    "        self.word2idx = {} # 单词 到 inx\n",
    "        self.embedding_matrix = [] # 词嵌入\n",
    "    def get_w2v_model(self):\n",
    "        # 把之前训练好的 word to vector 模型读取出来\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)# 加载模型\n",
    "        self.embedding_dim = self.embedding.vector_size # 获取维度\n",
    "    def add_embedding(self, word):\n",
    "        # 把 word 加进 embedding，并赋予他一个随机生成的represrntation vector\n",
    "        # word 只会是“<PAD>”或者“<UNK>”\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        # 创建一个未被初始化数值的tensor，形状是是size（1，词嵌入的维度）\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        # 从均匀分布（1，embedding_dim)中生成值\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        # 将长度放入self.word2idx中\n",
    "        self.idx2word.append(word)\n",
    "        # 将word添加到self.idx2word的末尾\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "        # 竖着将 embedding_matrix和vector拼接起来\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        # 取得训练好的 word2vex word embedding\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # 制作一个 word2idx的字典\n",
    "        # 制作一个 idx2word的列表\n",
    "        # 制作一个 word2vector的列表\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "        # enumerate()用于将词嵌入中单词组合为一个索引序列，同时列出数据和数据下标\n",
    "            print('get words #{}'.format(i+1), end='\\r')\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "         # 将“<PAD>”和“<UNK>”加入embedding中\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    def pad_sequence(self, sentence):\n",
    "         # 将每个句子变成一样的长度\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "            # 如果句子长则截断\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        #句子长度相等正常运行，句子长度不等触发异常\n",
    "        return sentence\n",
    "    def sentence_word2idx(self):\n",
    "       # 把句子里面的字转成相对应的 index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                    # 单词在句子中则将其放入索引之中\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 单词不在句子中则在索引中放入未知的标识\n",
    "            # 将每个句子变成一样的长度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "            # 将句子索引添加到列表的末尾\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把 labels 轉成 tensor\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "# 这部分所需要的“__init__\",\"__getitem__\",\"__len__\"好让dataloader能使用\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwitterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    期望的数据形状大致像：（数据数量，数据长度）\n",
    "    数据可以是numpy数组的list或者列表的list\n",
    "    输入数据的形状：（数据数量，序列长度，特征维度）\n",
    "    __len__将返回数据的数量\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "# 这个bolck是要拿来训练的模型\n",
    "import torch\n",
    "from torch import nn\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # 制作 embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否将embedding fix住，如果fix_embedding 为 False，在训练过程中，embedding也会跟着被训练\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用LSTM最后一层的隐藏状态\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "# 这个block是用来训练模型的\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train() # 将model的模式设为train，这样optimiser就看也i更新model的参数\n",
    "    criterion = nn.BCELoss() # 定义算是函数 这里使用binary cross entropy\n",
    "    t_batch = len(train) \n",
    "    v_batch = len(valid) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 将模型的参数给optimizer，并给与适当的learning rate\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 這段做 training\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            inputs = inputs.to(device, dtype=torch.long) \n",
    "            # device 为 “cuda”，将inputs转成torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) \n",
    "            # device 为“cuda”，将labels转成torch.duda.FloatTensor\n",
    "            optimizer.zero_grad() \n",
    "            # 由于loss.backward() 的 gradient 会累加，所以每次喂完一个batch后需要归零\n",
    "            outputs = model(inputs)  \n",
    "            # 将input 喂给模型\n",
    "            outputs = outputs.squeeze()  \n",
    "            # 去掉最外面的dimension，好让outputs 可以喂进criterion()\n",
    "            loss = criterion(outputs, labels) \n",
    "            # 计算此时模型的training loss\n",
    "            loss.backward() \n",
    "            # 计算loss的gradient\n",
    "            optimizer.step() \n",
    "            # 更新训练模型的参数\n",
    "            correct = evaluation(outputs, labels) \n",
    "            # 计算此时模型的training accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        # 这段做 validation\n",
    "        model.eval() #将model的模式设为eval，这样model的参数就会固定住\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long) \n",
    "                # decice为“cude\",将input转成torch.cuda.LongTensor\n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                # device为\"cude\",将labels转成torch cuda.FloatTensor\n",
    "                outputs = model(inputs) \n",
    "                # 將 input 放入模型中\n",
    "                outputs = outputs.squeeze()  \n",
    "                # 去掉最外面的 dimension，好让 outputs 可以放进 criterion()\n",
    "                loss = criterion(outputs, labels)  \n",
    "                # 计算此时模型的 validation loss\n",
    "                correct = evaluation(outputs, labels)  \n",
    "                # 计算此时模型的 validation accuracy\n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            if total_acc > best_acc:\n",
    "                 # 如果validation的结果优于之前的结果，就把当下的模型保存下来以备之后做预测时使用\n",
    "                best_acc = total_acc\n",
    "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 将model的模式设为train，这样optimizer就可以更新model的参数（因为刚刚转成eval模式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "# 这个 block 用来对testing_data.txt做预测\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1 # 大于等于0.5为正面\n",
    "            outputs[outputs<0.5] = 0 # 小于0.5为负面\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #24694\n",
      "total words: 24696\n",
      "sentence count #200000\n",
      "start training, parameter total:6415351, trainable:241351\n",
      "\n",
      "[ Epoch1: 1407/1407 ] loss:0.356 acc:21.094 \n",
      "Train | Loss:0.49712 Acc: 75.117\n",
      "Valid | Loss:0.45631 Acc: 77.956 \n",
      "saving model with acc 77.956\n",
      "-----------------------------------------------\n",
      "[ Epoch2: 1407/1407 ] loss:0.526 acc:19.531 \n",
      "Train | Loss:0.44308 Acc: 79.154\n",
      "Valid | Loss:0.44218 Acc: 78.856 \n",
      "saving model with acc 78.856\n",
      "-----------------------------------------------\n",
      "[ Epoch3: 1407/1407 ] loss:0.358 acc:20.312 \n",
      "Train | Loss:0.42594 Acc: 80.178\n",
      "Valid | Loss:0.43172 Acc: 79.553 \n",
      "saving model with acc 79.553\n",
      "-----------------------------------------------\n",
      "[ Epoch4: 1407/1407 ] loss:0.478 acc:18.750 \n",
      "Train | Loss:0.41360 Acc: 80.882\n",
      "Valid | Loss:0.42230 Acc: 80.295 \n",
      "saving model with acc 80.295\n",
      "-----------------------------------------------\n",
      "[ Epoch5: 1407/1407 ] loss:0.568 acc:17.969 \n",
      "Train | Loss:0.40203 Acc: 81.565\n",
      "Valid | Loss:0.42175 Acc: 80.165 \n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "path_prefix = './'\n",
    "# main.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 通过 torch.cuda.is_available() 的回传值进行判断是否有使用 GPU 的环境，如果有的话 device 就设为 \"cuda\"，没有的话就设为 \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 处理好各个 data 的路径\n",
    "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
    "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
    "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
    "\n",
    "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
    "\n",
    "# 定义句子长度、要不要固定 embedding、batch 大小、要训练几个 epoch、learning rate 的值、model 的资料夹路径\n",
    "sen_len = 20\n",
    "fix_embedding = True # 训练时fix住embedding\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "model_dir = path_prefix # 检查点模型的目录\n",
    "\n",
    "print(\"loading data ...\") \n",
    "# 把 'training_label.txt' 跟 'training_nolabel.txt' 读进来\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "train_x_no_label = load_training_data(train_no_label)\n",
    "\n",
    "# 对 input 跟 labels 做预处理\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "# 制作一个 model 的对象\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model = model.to(device) \n",
    "# device为 \"cuda\"，model 使用 GPU 来训练（喂进去的 inputs 也需要是 cuda tensor）\n",
    "\n",
    "# 把 data 分为 training data 跟 validation data（将一部份 training data 拿去当作 validation data）\n",
    "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
    "\n",
    "# 把 data 做成 dataset 供 dataloader 取用\n",
    "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "\n",
    "# 把 data 转成 batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "# 开始训练\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #24694\n",
      "total words: 24696\n",
      "sentence count #200001\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "# 开始测试模型并做预测\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data(testing_data)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "print('\\nload model ...')\n",
    "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
    "print(\"Finish Predicting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
